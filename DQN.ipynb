{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4: Deep Q-Learning  \n",
    "During class, we have learnt reinforcement learning. In this assignment, you need to design a Deep Q-Learning network to play the *CartPole* game. Our experiment consists of three parts:\n",
    "\n",
    "1. Introduction to CartPole game\n",
    "\n",
    "2. Implementation of Deep Q-Learning network\n",
    "\n",
    "3. Analytical questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us import some necessary packages.\n",
    "To run CartPole, you need to install the `gym` and `pyglet` package:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see animation while your code executes, you can:\n",
    "\n",
    "- Restart your kernel.\n",
    "\n",
    "- Alter the code as the comments in the last code block suggest.\n",
    "\n",
    "As making animation is very time-consuming, you may want to ensure your code is right before rendering the animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to CartPole\n",
    "[CartPole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) is a classical environment from [gym](https://gym.openai.com/) package that is designed by [OpenAI](https://openai.com/). In CartPole environment, a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum will fall over due to gravity if the cart doesn't move.\n",
    "\n",
    "![](./fig.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our goal is to balance the pole on the cart. In this experiment, we have:\n",
    "- Continuous state space: $(x, \\theta, \\dot{x}, \\dot{\\theta})$ \n",
    "    - $x$: position of the cart on the track\n",
    "    - $\\theta$: angle of the pole with the vertical\n",
    "    - $\\dot{x}$: cart velocity\n",
    "    - $\\dot{\\theta}$: rate of change of the angle\n",
    "\n",
    "- Discrete action space: force $F\\in\\{0, 1\\}$. $0$ for left force, $1$ for right force\n",
    "\n",
    "- Reward: $+5$ if the pole is upright, $-5$ if the pole is more than $15$ degrees from vertical, or the cart moves more than $2.4$ units from the center.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Deep Q-Learning\n",
    "\n",
    "Given state space $S$, action space $A$, and policy $\\pi$, the Q-value function $Q^\\pi(s, a)$ is defined as the expected payoff if we take action $a\\in A$ at state $s \\in S$ and follow $\\pi$:\n",
    "$$\n",
    "Q^{\\pi}(s, a)=\\mathbb{E}\\left[\\sum_{t \\geq 0} \\gamma^{t} r_{t} \\mid s_{0}=s, a_{0}=a, \\pi\\right],\n",
    "$$\n",
    "where $\\gamma$ is the discount factor, $r_t$ is the reward at time step $t$. So the optimal Q-value function is \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^{*}(s, a)&=\\max _{\\pi} Q^{\\pi}(s, a)=\\max _{\\pi} \\mathbb{E}\\left[\\sum_{t \\geq 0} \\gamma^{t} r_{t} \\mid s_{0}=s, a_{0}=a, \\pi\\right]\\\\&=\\mathbb{E}_{s^{\\prime} \\sim \\mathcal{E}}\\left[r+\\gamma \\max _{a^{\\prime}} Q^{*}\\left(s^{\\prime}, a^{\\prime}\\right) \\mid s_{0}=s, a_{0}=a\\right].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In deep Q-learning, we use a neural network $Q_{\\theta}$ to approximate $Q^{*}(s, a)$. Basically, the process can be described in following way:\n",
    "\n",
    "When we apply action $a_t$ at state $s_t$, the environment gives a feedback that consists of next state $s_{t+1}$ and reward $r_t$. So we get a sample $(s_t, a_t, r_t, s_{t+1})$. For the next step, we can get another sample $(s_{t+1}, a_{t+1}, r_{t+1}, s_{t+2})$ similarly. Apparently, these samples are highly related. Using these samples to train our model invalidates the i.i.d. assumption on training samples. Therefore, experience replay was proposed to reduce sample correlation by using random mini-batch sampled from a replay memory $\\mathcal D$. With an exploration rate $\\epsilon$, the exact algorithm is as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">Initialize replay memory $\\mathcal D$ to capacity $N$<br>\n",
    "Initialize the Q-value model $Q_\\theta$ with random weights<br> \n",
    "For episode $=1, \\dots, M$ do \\{<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Initialize state $s_1$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;For $t=1, \\dots, T$ do \\{<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With probability $\\epsilon$, select a random action $a_t$,<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;otherwise set $a_t = \\arg\\max_{a}Q_\\theta\\left(s_{t}, a \\right)$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Apply action $a_t$ and get reward $r_t$ and next state $s_{t+1}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Store transition $(s_t, a_t, r_t, s_{t+1})$ in memory $\\mathcal D$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sample random minibatch of transitions $\\Omega$ from $\\mathcal D$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each sample $(s_j, a_j, r_j, s_{j+1})$ in minibatch $\\Omega$ do \\{<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set $y_{j}=\\left\\{\\begin{array}{ll}\n",
    "r_{j},  &s_{j+1}\\text{ is a terminal state}\\\\\n",
    "r_{j}+\\gamma \\max _{a} Q_{\\theta} \\left(s_{j+1}, a\\right),&s_{j+1}\\text{ is not a terminal state}\n",
    "\\end{array}\\right.$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Perform a gradient descent step using $\\left(y_{j}-Q_{\\theta}\\left(s_{j}, a_{j}\\right)\\right)^{2}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\}<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\\}<br>\n",
    "\\}<br>\n",
    "\n",
    "We will use a *deque* object as memory $\\mathcal D$ to store transitions. \n",
    "\n",
    "Here, we are going to re-use the simple neural network we build in PA3, `SimpleNN`, to build a **q-model** for deep Q-learning. \n",
    "\n",
    "You are now provided a python file named `nn.py`, so you don't need to copy your codes from PA3. Now, let us import `nn` and `deque` so we can use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from nn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can set other parameters and construct a *deque* object as our memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "gamma = 0.95    # discount rate\n",
    "epsilon = 1.0  # exploration rate\n",
    "epsilon_min = 0.01 # lower bound of exploration\n",
    "epsilon_decay = 0.995 # epsilon decay rate\n",
    "N = 2000 # capacity of memory\n",
    "M = 1000 # maximum of episode\n",
    "T = 500 # maximum of time step t\n",
    "memory = deque(maxlen=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are only two possible actions ($0$ or $1$), we can directly predict the Q-values of applying two actions at a state instead of using action as a part of input features:\n",
    "$$\n",
    "Q_\\theta(s) \\approx \\begin{bmatrix}Q^*(s, 0) \\\\ Q^*(s, 1) \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "To achieve it, we just need to set the output dimension to $2$. The q-model takes states as input, so the input dimension is set to $4$.  Other parts can remain the same as PA3, so our q-model contains:\n",
    "- L0, Input layer (shape: $N \\times 4$), where $N$ is the batch size.\n",
    "- L1, Linear layer (shape: $4 \\times 80$)\n",
    "- L2, ReLU layer\n",
    "- L3, Linear layer (shape: $80 \\times 80$)\n",
    "- L4, ReLU layer\n",
    "- L5, Output layer (linear layer, shape: $80 \\times 2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartpole game\n",
    "env = gym.make('CartPole-v1')\n",
    "#### info related to state and actions\n",
    "# the length of state vector, and state is continuous\n",
    "state_vec_len = env.observation_space.shape[0]\n",
    "# the size of action set\n",
    "action_size = env.action_space.n\n",
    "layers = [ \n",
    "    # input layer is input data\n",
    "    # L1\n",
    "    Linear(state_vec_len, 80),\n",
    "    # L2\n",
    "    ReLU(),\n",
    "    # L3\n",
    "    Linear(80, 80),\n",
    "    # L4\n",
    "    ReLU(),\n",
    "    # L5\n",
    "    Linear(80, action_size)\n",
    "]\n",
    "loss = MSELoss()\n",
    "# sgd: lr = 0.0005\n",
    "# bgd: lr = 0.001\n",
    "q_model = SimpleNN(layers, loss, lr=0.001)   # generate nets and DQNagent model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each time step, we need to apply an action at current state. Here, we define a function `act` to select the action. The `act` function takes current state as input and returns the selected action. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 (2 points). Please complete `act` with your implementation to select an action at given state. The corresponding pseudocode is listed as follows. *Hint: Use if-else statement*.\n",
    "\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With probability $\\epsilon$, select a random action $a_t$,<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;otherwise set $a_t = \\arg\\max_{a}Q_\\theta\\left(s_{t}, a \\right)$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select an action.\n",
    "\n",
    "def act(state):\n",
    "    ### start of your codes\n",
    "\n",
    "    ### end of your codes\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying an action, the environment will give us the next state and reward. In our situation, it also tell us if the state is terminal. We use the `remember` function to store current transition in memory $\\mathcal D$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(memory, state, action, reward, next_state, terminal):\n",
    "    memory.append((state, action, reward, next_state, terminal))\n",
    "    return memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can randomly sample a minibatch from $\\mathcal D$ to train our q-model. The process is called *experience replay*. Here we define a `replay` function to do this. The input parameter of `replay` is memory $\\mathcal D$, our q-model, and batch_size. After training, `replay` returns the updated q-model.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 (4 points). Please complete `replay` to train our q-model. The corresponding pseudocode is listed as follows. *Hint: For q-model, if you are using SGD, the recommended learning rate is 0.0005; if you are using BGD, the recommended learning rate is 0.001*.\n",
    "\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sample random minibatch of transitions $\\Omega$ from $\\mathcal D$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each sample $(s_j, a_j, r_j, s_{j+1})$ in minibatch $\\Omega$ do \\{<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set $y_{j}=\\left\\{\\begin{array}{ll}\n",
    "r_{j} ,&s_{j+1}\\text{ is a terminal state}\\\\\n",
    "r_{j}+\\gamma \\max _{a} Q_{\\theta}\\left(s_{j+1}, a\\right),&s_{j+1}\\text{ is not a terminal state}\n",
    "\\end{array}\\right.$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Perform a gradient descent step using $\\left(y_{j}-Q_{\\theta}\\left(s_{j}, a_{j}\\right)\\right)^{2}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\}<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(memory, q_model, batch_size):\n",
    "    # sample random minibatch:\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    # extract corresponding states (s_j), actions (a_j), rewards (r_j), next_states (s_{j+1}), and terminal state indicator from the sampled minibatch:\n",
    "    states = np.array([minibatch[i][0] for i in range(batch_size)]) \n",
    "    actions = np.array([minibatch[i][1] for i in range(batch_size)]).astype(int)\n",
    "    rewards = np.array([minibatch[i][2] for i in range(batch_size)]) \n",
    "    next_states = np.array([minibatch[i][3] for i in range(batch_size)]) \n",
    "    terminal = np.array([minibatch[i][4] for i in range(batch_size)]).astype(float)\n",
    "    # perform the gradient descent step by implementing the formula in pseudocode:\n",
    "    # start of your codes\n",
    "    # step1: obtain maximum Q (Q_theta)\n",
    "\n",
    "    # step2: calculate y_j\n",
    "\n",
    "    # step3: update y by y_j at corresponding index\n",
    "\n",
    "    # setp4: backward\n",
    "\n",
    "    # end of your codes\n",
    "    return q_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieved the majority of our deep Q-learning network. In the following code, we set up the game environment to get feedback based on the current state and action. \n",
    "\n",
    "The above functions you implemented will be evaluated by a score `score_dqn`, which is just the number of time steps the agent plays in  one episode  (episode ends when `terminal=True`). We will compute the mean score after a certain number of episodes and visualize the results. \n",
    "\n",
    "A baseline of random action selection `score_random` is computed so you can compare the performance of your DQN with the baseline. Also, the log of each episode is saved in `log.txt` where you can find the scores of all episodes. After each episode, the exploration rate decreases.\n",
    "\n",
    "\n",
    "Run the following code and good luck! Please attach the figure and log into your submission. DO NOT contain all of your logs!\n",
    "\n",
    "*Hint: After about 200 episodes, your score should oscillate around 150. It is normal to occasionally get extremely low scores in the later stages of training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for visualization\n",
    "\n",
    "# for deep q-learning\n",
    "mean_score_dqn = []\n",
    "score_dqn = 0\n",
    "# for random action\n",
    "mean_score_random = []\n",
    "score_random = 0\n",
    "# visualize the mean score of each plot_n steps\n",
    "plot_n = 10\n",
    "batch_size = 100\n",
    "currentDateAndTime = datetime.now()\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for episode in range(M):\n",
    "    state_dqn = env.reset() \n",
    "    if len(state_dqn) == 2:\n",
    "        state_dqn = state_dqn[0]\n",
    "    for time in range(T):\n",
    "        # env.render()                # show the amination. Comment this line to speed up\n",
    "        action = act(state_dqn)     # chose action\n",
    "        # to get feedback:\n",
    "        next_state, reward, terminal, _, _ = env.step(action) # if you are NOT using env.render() to speed up\n",
    "        # next_state, reward, terminal, _ = env.step(action) # if you are using env.render() to create animation\n",
    "        reward = 5 if not terminal else -5  # get -5 reward if fail\n",
    "        memory = remember(memory, state_dqn, action, reward, next_state, terminal) # store transition\n",
    "        state_dqn = next_state\n",
    "        if len(memory) > batch_size and time % 5 == 0: # train q_model\n",
    "            q_model = replay(memory, q_model, batch_size)\n",
    "        if terminal:                \n",
    "            print(\"episode: {}/{}, score: {}, epsilon: {:.2}\"\n",
    "                  .format(episode, M, time, epsilon), file=open('./log.txt', 'a'))\n",
    "            break\n",
    "    score_dqn += time\n",
    "\n",
    "    # random action\n",
    "    state_random = env.reset()\n",
    "    for time in range(500):\n",
    "        action = random.randint(0, action_size-1)\n",
    "        # to get feedback:\n",
    "        next_state, reward, terminal, _, _ = env.step(action) # if you are NOT using env.render() to speed up\n",
    "        # next_state, reward, terminal, _ = env.step(action) # if you are using env.render() to create animation\n",
    "        state_random = next_state\n",
    "        if terminal:                \n",
    "            break\n",
    "    score_random += time\n",
    "    \n",
    "    # plot training process\n",
    "    if (episode+1) % plot_n == 0:\n",
    "        display.clear_output()\n",
    "        plt.clf()\n",
    "        mean_score_dqn.append(score_dqn / plot_n)\n",
    "        mean_score_random.append(score_random / plot_n)\n",
    "        score_dqn = 0\n",
    "        score_random = 0\n",
    "        plt.title('expisode={}'.format(episode+1))\n",
    "        plt.plot(mean_score_dqn, label=\"DQN\")\n",
    "        plt.plot(mean_score_random, label=\"random\")\n",
    "        plt.legend()\n",
    "        display.display(plt.gcf())\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Question\n",
    "\n",
    "\n",
    "#### Q3. (Short Answer, 2 points) We have adopted a dynamic exploration rate `epsilon` when agent chooses an action in this experiment. Please explain how the exploration rate is controlled in the above code and why it works.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer:* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Questions\n",
    "\n",
    "*Note: Your discussion needs to be supported by numerical results or figures*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4 (1 point). Experiment with different strategies of the exploration-exploitation trade off  (e.g. let `epsilon` be 0, constant or dynamic with different initial values) and discuss their difference.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5 (1 point). Try different values in the reward function. Compare the results and analyze the relationship between reward function and the learned results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6 (2 points). Try different random seeds, and try running your network with a same seed multiple times. To what extent are the results and convergence time affected? Is deep reinforcement learning robust to randomness of environments? If not, what may be the cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer:* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
